{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "paths = [\"../data/img/vit/train/0_real/\", \"../data/img/vit/train/1_fake/\", \"../data/img/vit/val/0_real/\", \"../data/img/vit/val/1_fake/\"]\n",
    "\n",
    "problematic = []\n",
    "\n",
    "for path in paths:\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            img = Image.open(path + filename)\n",
    "            if img.mode != \"RGB\":\n",
    "                problematic.append(path + filename)\n",
    "                print(path + filename)\n",
    "                img = img.convert(\"RGB\")\n",
    "                img.save(path + filename)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 6910/6910 [00:00<00:00, 125127.43it/s]\n",
      "Resolving data files: 100%|██████████| 1728/1728 [00:00<00:00, 850376.31it/s]\n",
      "Downloading data files: 100%|██████████| 6910/6910 [00:00<00:00, 178046.83it/s]\n",
      "Downloading data files: 0it [00:00, ?it/s]\n",
      "Extracting data files: 0it [00:00, ?it/s]\n",
      "Downloading data files: 100%|██████████| 1728/1728 [00:00<00:00, 164430.27it/s]\n",
      "Downloading data files: 0it [00:00, ?it/s]\n",
      "Extracting data files: 0it [00:00, ?it/s]\n",
      "Generating train split: 6910 examples [00:00, 41070.81 examples/s]\n",
      "Generating validation split: 1728 examples [00:00, 40363.31 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 6910\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 1728\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=\"../data/img/vit\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "\n",
    "# # Assuming you're accessing the first image in the training set\n",
    "# for i in range(len(dataset['train'])):\n",
    "#   sample_image = dataset['train'][i]['image']\n",
    "\n",
    "#   # If the image is a PIL Image\n",
    "#   if isinstance(sample_image, Image.Image):\n",
    "#       print(f\"Image mode: {sample_image.mode}\")\n",
    "#       if sample_image.mode == 'RGB':\n",
    "#           print(\"The image has 3 channels (RGB).\")\n",
    "#       elif sample_image.mode == 'L':\n",
    "#           print(i + 1)\n",
    "#           print(\"The image has 1 channel (grayscale).\")\n",
    "#           break\n",
    "\n",
    "#   # If the image is a NumPy array\n",
    "#   elif isinstance(sample_image, np.ndarray):\n",
    "#       print(f\"Image shape: {sample_image.shape}\")\n",
    "#       if len(sample_image.shape) == 3 and sample_image.shape[2] == 3:\n",
    "#           print(\"The image has 3 channels (RGB).\")\n",
    "#           break\n",
    "#       elif len(sample_image.shape) == 2:\n",
    "#           print(\"The image has 1 channel (grayscale).\")\n",
    "#           break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTImageProcessor {\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"image_processor_type\": \"ViTImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"resample\": 2,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 224,\n",
       "    \"width\": 224\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import ViTImageProcessor\n",
    "\n",
    "model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
    "processor = ViTImageProcessor.from_pretrained(model_name_or_path)\n",
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(batch):\n",
    "  # Take a list of PIL images and turn them to pixel values\n",
    "  inputs = processor([x for x in batch['image']], return_tensors=\"pt\")\n",
    "\n",
    "  # Include labels\n",
    "  inputs['labels'] = batch['label']\n",
    "  return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset[\"train\"].with_transform(transform)\n",
    "validation = dataset[\"validation\"].with_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(batch):\n",
    "  return {\n",
    "    'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "    'labels': torch.tensor([x['labels'] for x in batch])\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "  logits, labels = eval_pred\n",
    "  predictions = np.argmax(logits, axis=-1)\n",
    "  return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(model_name_or_path, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "epochs = 2\n",
    "warmup_steps = 100\n",
    "weight_decay = 0.01\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir='./results',\n",
    "  num_train_epochs=epochs,\n",
    "  per_device_train_batch_size=32,\n",
    "  per_device_eval_batch_size=16,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  warmup_steps=warmup_steps,\n",
    "  weight_decay=weight_decay,\n",
    "  logging_dir='./logs',\n",
    "  remove_unused_columns=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  train_dataset=train,\n",
    "  eval_dataset=validation,\n",
    "  tokenizer=processor,\n",
    "  data_collator=collate_fn,\n",
    "  compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./model_vit\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 432/432 [11:13<00:00,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 673.7471, 'train_samples_per_second': 20.512, 'train_steps_per_second': 0.641, 'train_loss': 0.1771984100341797, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=432, training_loss=0.1771984100341797, metrics={'train_runtime': 673.7471, 'train_samples_per_second': 20.512, 'train_steps_per_second': 0.641, 'train_loss': 0.1771984100341797, 'epoch': 2.0})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./model_transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViTForImageClassification.from_pretrained(\"./model_transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rs/hvbh7zq16zz6m2qllm8l0_080000gn/T/ipykernel_86970/645465551.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(processor(dataset['validation'][i]['image'], return_tensors=\"pt\")['pixel_values'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 103\n",
    "input = torch.tensor(processor(dataset['validation'][i]['image'], return_tensors=\"pt\")['pixel_values'])\n",
    "\n",
    "outputs = model(input)\n",
    "predictions = outputs.logits.argmax(dim=-1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
